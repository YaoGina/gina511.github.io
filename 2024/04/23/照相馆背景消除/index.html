
<!DOCTYPE html>
<html lang="en">
    <!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  
    <link rel="icon" href="/gina511.github.io/img/favicon.png">
  
  
      <meta name="author" content="Gina">
  
  
  
  
  
    <link rel="alternate" href="/gina511.github.io/atom.xml " title="Gina" type="application/atom+xml">
  

  

  <title>Carvana ImageMasking Challenge  [基于UNET的kaggle图像遮蔽挑战] | Gina</title>

  

  

  

  <link rel="stylesheet" href="/gina511.github.io/css/style.css" >
  <link rel="stylesheet" href="/gina511.github.io/css/partial/dark.css" >

  
  
  

  
    
      <link rel="stylesheet" href="/gina511.github.io/css/partial/highlight/atom-one-light.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/a2396837/CDN@latest/css/iconfont.css">
    
  

  
    <script src="/gina511.github.io/js/todark.js"></script>
    
<meta name="generator" content="Hexo 7.1.1"></head>
</html>
    
<div class="nav index" style="height: 60px;">
    <div class="title animated fadeInDown">
        <div class="layui-container">
                <div class="nav-title"><a href="/gina511.github.io/" title="Gina">Gina</a></div>
            <div class="nav-list">
                <button> <span class=""></span><span style="display: block;"></span><span class=""></span> </button>
                <ul class="layui-nav" lay-filter="">
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/ ">
                            <i class=" fab fa-fort-awesome " style="color: rgb(255 107 107);"></i>
                            <span class="layui-nav-item-name">首页</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/archives/ ">
                            <i class=" fas fa-archive " style="color: rgb(10 189 227);"></i>
                            <span class="layui-nav-item-name">列表</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/tags ">
                            <i class=" fas fa-hashtag " style="color: rgb(254 202 87);"></i>
                            <span class="layui-nav-item-name">标签</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/categories ">
                            <i class=" far fa-folder-open " style="color: rgb(29 209 161);"></i>
                            <span class="layui-nav-item-name">分类</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/profile/湖南大学人工智能专业姚姬娜.pdf ">
                            <i class=" fab fa-grav " style="color: rgb(154 106 247);"></i>
                            <span class="layui-nav-item-name">关于</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/link/ ">
                            <i class=" fab fa-weixin " style="color: hsl(152deg 73% 45%);"></i>
                            <span class="layui-nav-item-name">友链</span>
                        </a>
                    </li>
                    
                        
                        
                        
                        
                    <li class="layui-nav-item">
                        <a href="/gina511.github.io/shuoshuo/ ">
                            <i class=" fas fa-coffee " style="color:#31c7c1;"></i>
                            <span class="layui-nav-item-name">说说</span>
                        </a>
                    </li>
                    
                    
                        <li class="layui-nav-item" id="btn-toggle-dark">🌙</li>
                    
                    <span class="layui-nav-bar" style="left: 342px; top: 78px; width: 0px; opacity: 0;"></span>
                </ul>
            </div>
        </div>
    </div>
</div>
    
<header class="header">
        
            <div class="logo">
                    <a href="/gina511.github.io/"><img src="img/header.png" onerror=this.onerror=null,this.src="/gina511.github.io/img/loading.gif"></a>
            </div>
         
    </div>
     

            <div class="motto">
                <span>为你，千千万万遍</span>
            </div>
    
    
            <div class="social">
                
                        <a class="social-icon" href="https://yaogina.github.io/gina511.github.io/" target="_blank" title="Github">
                            <i class="iconfont icon-GitHub" aria-hidden="true"></i>
                          </a>
                 
                        <a class="social-icon" href="gina2021@hnu.edu.cn" target="_blank" title="Email">
                            <i class="iconfont icon-email" aria-hidden="true"></i>
                          </a>
                 
            </div>
     
</header>

    
<article id="post">
  <div class="post-title">Carvana ImageMasking Challenge  [基于UNET的kaggle图像遮蔽挑战]</div>
  
<div class="post-meta">
    
    
      <div class="post-meta-item date">
        <span title="Created 2024.04.23"><i class="far fa-calendar-alt"></i> 2024.04.23</span>
      </div>
      <div class="post-meta-item updated">
        <span title="Updated 2024.05.26"><i class="far fa-calendar-check"></i> 2024.05.26</span>
      </div>
     
    
      <div class="post-meta-item categories">
        
          <i class="fas fa-inbox article-meta__icon"></i> <a href="/gina511.github.io/categories/cv/">cv</a>
        
      </div>
     
    
     <div class="post-meta-item wordcount">
        
          <i class="fas fa-pencil-alt"></i> <span class="post-count">4.3k words</span>
           
        
          <i class="far fa-clock"></i> <span class="post-count">17 min</span>
                               
      </div>
     
</div>


  
  <div class="content">
        <div><h1 id="Carvana-ImageMasking-Challenge"><a href="#Carvana-ImageMasking-Challenge" class="headerlink" title="Carvana ImageMasking Challenge"></a>Carvana ImageMasking Challenge</h1><h3 id="一、实验要求"><a href="#一、实验要求" class="headerlink" title="一、实验要求"></a>一、实验要求</h3><p>自动识别图像中汽车的边界。</p>
<p>要求：开发一种自动删除照相馆背景的算法。</p>
<span id="more"></span>



<h3 id="二、实验思路"><a href="#二、实验思路" class="headerlink" title="二、实验思路"></a>二、实验思路</h3><h4 id="一、解析问题"><a href="#一、解析问题" class="headerlink" title="一、解析问题"></a>一、解析问题</h4><p>​		什么是图像分割问题呢？ 简单的来讲就是给一张图像，检测是用框出框出物体，而图像分割分出一个物体的准确轮廓。也这样考虑，给出一张图像 Ｉ，这个问题就是求一个函数，从I映射到Mask。所以这里kaggle的比赛名称也叫做 ImageMasking Challenge 。</p>
<h4 id="二、解决问题"><a href="#二、解决问题" class="headerlink" title="二、解决问题"></a>二、解决问题</h4><p>​		看了一下kaggle网站上对这个问题的解决方式，发现大部分都是使用unet解决，使用CNN的效果都很差，其余网络的提及也很少。UNET相比于CNN更适用于图像分割，因为它的编码-解码结构和跳跃连接机制使得网络能够同时学习到全局信息和局部细节，并且能够利用不同层次的特征进行语义信息传递，提高分割准确性。</p>
<p>​	   <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.04597v1">U-NET原论文地址</a>U-net是一个用于医学图像分割的全卷积神经网络。目前很多神经网络的输出结果都是最终的分类类别标签，但对医学影像的处理，医务人员除了想要知道图像的类别以外，更想知道的是图像中各种组织的位置分布，而U-net就可以实现图片像素的定位，该网络对图像中的每一个像素点进行分类，最后输出的是根据像素点的类别而分割好的图像。</p>
<h4 id="三、U-net结构"><a href="#三、U-net结构" class="headerlink" title="三、U-net结构"></a>三、U-net结构</h4><p><img src= "/gina511.github.io/img/loading.gif" data-src="/gina511.github.io/images/post-image/kaggle-masking-report/image-20240524162510686.png" alt="image-20240524162510686"></p>
<p>​	<strong>编码器部分负责对输入图像进行多次下采样，以提取图像的抽象特征。常用的下采样方法是使用卷积层和池化层，逐渐减小特征图的尺寸同时增加通道数。这样可以帮助网络学习到更高级别的语义信息，并减小特征图的空间维度。</strong> </p>
<ul>
<li>​	该U-net网络一共有四层，分别对图片进行了4次下采样和4次上采样。左侧为编码器进行下采样的过程，输入的是一张572×572×1的图片，然后经过<strong>64</strong>个3×3的卷积核进行卷积，再通过ReLU函数后得到<strong>64个570×570×1的特征通道</strong>。然后把这570×570×64的结果再经过64个3×3的卷积核进行卷积，同样通过ReLU函数后得到64个568×568×1的特征提取结果，这就是第一层的处理结果。</li>
<li>​	之后重复四次进行下采样，每下采样一次就会把图片的大小减小一般，卷积核层数增加一倍，这样处理的图片大小越来越小，而特征也会提取到不同的特征通道中。</li>
</ul>
<p>​	<strong>解码器部分是编码器的镜像，负责将编码器提取的特征还原到原始尺寸，并从不同层次融合特征，生成最终的分割结果。这里采用了上采样和卷积操作，逐渐恢复特征图的尺寸，同时进行特征融合和信息传递。</strong> </p>
<ul>
<li>​	右边部分从下往上则是4次上采样过程。从最右下角开始，把28×28×1024的特征矩阵经过<strong>512个</strong>2×2的卷积核进行反卷积，把矩阵扩大为56×56×512，<strong>由于反卷积只能扩大图片而不能还原图片</strong>，为了减少数据丢失，采取把左边降采样时的图片裁剪成相同大小后直接拼过来的方法增加特征层，再进行卷积来提取特征。</li>
<li>​	最终不断重复，每次上采样都会让图像大小扩大一倍而卷积核层数减少，最终获得两层特征输出，其实相当于做了二分类的过程，输出的分别是一层背景和一层轮廓目标。</li>
</ul>
<p>​	UNET使用跳跃连接来连接编码器和解码器的对应层。跳跃连接可以帮助网络进行多尺度信息的融合，提供了更丰富的上下文信息，并避免了信息丢失。通过跳跃连接，UNET能够将低层次的细节特征与高层次的语义特征相结合，提高图像分割的准确性。 在训练过程中，UNET通常采用交叉熵损失函数来度量分割结果与真实标签的差异，并通过反向传播算法更新网络的权重。此外，UNET还可采用数据增强技术来增加训练样本的多样性，防止过拟合。</p>
<h3 id="三、实验过程"><a href="#三、实验过程" class="headerlink" title="三、实验过程"></a>三、实验过程</h3><h4 id="A-u-net实现"><a href="#A-u-net实现" class="headerlink" title="A.u-net实现"></a>A.u-net实现</h4><p>   Keras是一个高级神经网络API，它可以用于构建、训练和部署深度学习模型。Keras是建立在底层深度学习库（如TensorFlow、Theano等）之上的，它提供了简化和高级抽象的接口，使得模型的构建和训练过程更加方便和快捷。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_unet_128</span>(<span class="params">input_shape=(<span class="params"><span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span></span>),    <span class="comment">#分成rgb三个通道</span></span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1</span></span>):</span><br><span class="line">    inputs = Input(shape=input_shape)</span><br><span class="line">    <span class="comment"># 128</span></span><br><span class="line">    down1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(inputs)   <span class="comment">#使用3x3的卷积核对输入图像进行卷积操作，生成64个通道的特征图。</span></span><br><span class="line">    down1 = BatchNormalization()(down1)<span class="comment">#应用批量归一化，加速模型收敛和提高模型的泛化能力。</span></span><br><span class="line">    down1 = Activation(<span class="string">&#x27;relu&#x27;</span>)(down1)<span class="comment">#激活函数ReLU，引入非线性特征。</span></span><br><span class="line">    down1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(down1) <span class="comment">#再次使用3x3的卷积核对特征图进行卷积操作，保持通道数不变。</span></span><br><span class="line">    down1 = BatchNormalization()(down1)</span><br><span class="line">    down1 = Activation(<span class="string">&#x27;relu&#x27;</span>)(down1)</span><br><span class="line">    down1_pool = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(down1)<span class="comment">#使用2x2的池化核进行下采样，将图像尺寸缩小一半，并保留每个网格中的最大值。这样做可以减少特征图的空间维度，同时保留主要的特征。</span></span><br><span class="line">    <span class="comment"># 64  32  16  8</span></span><br><span class="line">    <span class="comment">#其余下采样过程省略</span></span><br><span class="line">    center = Conv2D(<span class="number">1024</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(down4_pool)</span><br><span class="line">    center = BatchNormalization()(center)</span><br><span class="line">    center = Activation(<span class="string">&#x27;relu&#x27;</span>)(center)</span><br><span class="line">    center = Conv2D(<span class="number">1024</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(center)</span><br><span class="line">    center = BatchNormalization()(center)</span><br><span class="line">    center = Activation(<span class="string">&#x27;relu&#x27;</span>)(center)</span><br><span class="line">    <span class="comment"># center</span></span><br><span class="line">    up4 = UpSampling2D((<span class="number">2</span>, <span class="number">2</span>))(center)  <span class="comment">#使用2x2的上采样核将特征图的尺寸放大一倍。</span></span><br><span class="line">    up4 = concatenate([down4, up4], axis=<span class="number">3</span>)  <span class="comment">#将上采样后的特征图与相应的下采样结果进行拼接，以融合低层次和高层次的特征信息。</span></span><br><span class="line">    up4 = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(up4)  <span class="comment">#使用3x3的卷积核对特征图进行卷积操作，生成512个通道的特征图。</span></span><br><span class="line">    up4 = BatchNormalization()(up4)</span><br><span class="line">    up4 = Activation(<span class="string">&#x27;relu&#x27;</span>)(up4)</span><br><span class="line">    up4 = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(up4)</span><br><span class="line">    up4 = BatchNormalization()(up4)</span><br><span class="line">    up4 = Activation(<span class="string">&#x27;relu&#x27;</span>)(up4)</span><br><span class="line">    up4 = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>)(up4)</span><br><span class="line">    up4 = BatchNormalization()(up4)</span><br><span class="line">    up4 = Activation(<span class="string">&#x27;relu&#x27;</span>)(up4)</span><br><span class="line">    <span class="comment"># 16  32 64 128</span></span><br><span class="line">    <span class="comment"># 其余上采样过程省略</span></span><br><span class="line">    classify = Conv2D(num_classes, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)(up1)  <span class="comment">#使用1x1的卷积核对特征图进行卷积操作，将通道数调整为类别数（这里是1），并通过Sigmoid激活函数输出每个像素属于目标类别的概率。</span></span><br><span class="line">    model = Model(inputs=inputs, outputs=classify)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=RMSprop(lr=<span class="number">0.0001</span>), loss=bce_dice_loss, metrics=[dice_coeff])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>​	这里的代码包括了下采样路径和上采样路径，通过跳跃连接来保留下采样过程中的细节信息，并最终输出分割结果。网络的输入是一个大小为128x128x3的图像，输出是一个大小为128x128x1的二值分割图。</p>
<h4 id="B-loss"><a href="#B-loss" class="headerlink" title="B.loss"></a>B.loss</h4><p>​	在Unet的实现中将损失函数作为参数传入进行模型训练，这里介绍损失函数的具体实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=RMSprop(lr=<span class="number">0.0001</span>), loss=bce_dice_loss, metrics=[dice_coeff])</span><br></pre></td></tr></table></figure>

<p>Dice系数是一种常用的分割任务评估指标，也可以作为损失函数使用。它衡量了预测结果与真值之间的重叠程度。函数中，y_true表示真实标签，y_pred表示预测标签。平滑因子smooth用于避免分母为0，将真实标签和预测标签展平后计算它们的交集（相乘），然后计算Dice系数.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dice_coeff</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    smooth = <span class="number">1.</span></span><br><span class="line">    y_true_f = K.flatten(y_true)</span><br><span class="line">    y_pred_f = K.flatten(y_pred)</span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(y_true_f * y_pred_f)</span><br><span class="line">    score = (<span class="number">2.</span> * intersection + smooth) / (K.<span class="built_in">sum</span>(y_true_f) + K.<span class="built_in">sum</span>(y_pred_f) + smooth)</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>

<p>dice_loss函数:   <strong>1 - dice_coeff</strong>  ，即真值和预测之间的差异越小，loss越小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dice_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    loss = <span class="number">1</span> - dice_coeff(y_true, y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p>bce_dice_loss函数：将二元交叉熵损失和Dice Loss结合的一种损失函数。它考虑了两个方面，即像素级别的分类误差和分割结果的相似性。损失函数是二元交叉熵损失与Dice Loss之和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bce_dice_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<h4 id="c-图像增强"><a href="#c-图像增强" class="headerlink" title="c.图像增强"></a>c.图像增强</h4><p>这一节介绍对数据集的图像增强处理方法，增强模型的抗干扰性和泛化能力。</p>
<h5 id="1-RGB—-HSV"><a href="#1-RGB—-HSV" class="headerlink" title="1. RGB—-&gt;HSV"></a>1. RGB—-&gt;HSV</h5><p>首先，代码通过cv2.cvtColor将RGB图像转换为HSV颜色空间，HSV表示色调（Hue）、饱和度（Saturation）和明度（Value）。然后，代码根据输入的参数hue_shift_limit、sat_shift_limit和val_shift_limit随机生成色调、饱和度和明度的偏移量。这些参数确定了色调、饱和度和明度的变化范围。 接着，代码分别对色调、饱和度和明度进行偏移操作，使用cv2.add函数将生成的偏移量加到对应的通道上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)</span><br><span class="line">      h, s, v = cv2.split(image)</span><br><span class="line">      hue_shift = np.random.uniform(hue_shift_limit[<span class="number">0</span>], hue_shift_limit[<span class="number">1</span>])</span><br><span class="line">      h = cv2.add(h, hue_shift)</span><br><span class="line">      sat_shift = np.random.uniform(sat_shift_limit[<span class="number">0</span>], sat_shift_limit[<span class="number">1</span>])</span><br><span class="line">      s = cv2.add(s, sat_shift)</span><br><span class="line">      val_shift = np.random.uniform(val_shift_limit[<span class="number">0</span>], val_shift_limit[<span class="number">1</span>])</span><br><span class="line">      v = cv2.add(v, val_shift)</span><br><span class="line">      image = cv2.merge((h, s, v))</span><br><span class="line">      image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)</span><br></pre></td></tr></table></figure>

<h5 id="2-随机平移，缩放，旋转"><a href="#2-随机平移，缩放，旋转" class="headerlink" title="2. 随机平移，缩放，旋转"></a>2. 随机平移，缩放，旋转</h5><p>这段代码通过随机的平移、缩放和旋转来改变输入图像的外观，以增强训练数据的多样性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">height, width, channel = image.shape</span><br><span class="line">        angle = np.random.uniform(rotate_limit[<span class="number">0</span>], rotate_limit[<span class="number">1</span>])  <span class="comment"># degree</span></span><br><span class="line">        scale = np.random.uniform(<span class="number">1</span> + scale_limit[<span class="number">0</span>], <span class="number">1</span> + scale_limit[<span class="number">1</span>])</span><br><span class="line">        aspect = np.random.uniform(<span class="number">1</span> + aspect_limit[<span class="number">0</span>], <span class="number">1</span> + aspect_limit[<span class="number">1</span>])</span><br><span class="line">        sx = scale * aspect / (aspect ** <span class="number">0.5</span>)</span><br><span class="line">        sy = scale / (aspect ** <span class="number">0.5</span>)</span><br><span class="line">        dx = <span class="built_in">round</span>(np.random.uniform(shift_limit[<span class="number">0</span>], shift_limit[<span class="number">1</span>]) * width)</span><br><span class="line">        dy = <span class="built_in">round</span>(np.random.uniform(shift_limit[<span class="number">0</span>], shift_limit[<span class="number">1</span>]) * height)</span><br><span class="line"></span><br><span class="line">        cc = np.math.cos(angle / <span class="number">180</span> * np.math.pi) * sx</span><br><span class="line">        ss = np.math.sin(angle / <span class="number">180</span> * np.math.pi) * sy</span><br><span class="line">        rotate_matrix = np.array([[cc, -ss], [ss, cc]])</span><br><span class="line"></span><br><span class="line">        box0 = np.array([[<span class="number">0</span>, <span class="number">0</span>], [width, <span class="number">0</span>], [width, height], [<span class="number">0</span>, height], ])</span><br><span class="line">        box1 = box0 - np.array([width / <span class="number">2</span>, height / <span class="number">2</span>])</span><br><span class="line">        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / <span class="number">2</span> + dx, height / <span class="number">2</span> + dy])</span><br><span class="line"></span><br><span class="line">        box0 = box0.astype(np.float32)</span><br><span class="line">        box1 = box1.astype(np.float32)</span><br><span class="line">        mat = cv2.getPerspectiveTransform(box0, box1)</span><br></pre></td></tr></table></figure>

<h5 id="3-随机水平翻转"><a href="#3-随机水平翻转" class="headerlink" title="3. 随机水平翻转"></a>3. 随机水平翻转</h5><p>这段代码实现了随机水平翻转图像的操作。它接受输入图像和对应的掩膜（mask），然后根据给定的概率<code>u</code>决定是否进行水平翻转操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">randomHorizontalFlip</span>(<span class="params">image, mask, u=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; u:</span><br><span class="line">        image = cv2.flip(image, <span class="number">1</span>)</span><br><span class="line">        mask = cv2.flip(mask, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br></pre></td></tr></table></figure>

<h4 id="d-生成器"><a href="#d-生成器" class="headerlink" title="d.生成器"></a>d.生成器</h4><p>使用一个无限循环来生成训练数据和验证数据的批次。对于每个批次，它从<code>ids_train_split</code>列表中选择一部分训练样本的ID，并依次处理每个ID。对于每个ID，它读取对应的图像文件和掩码文件，并进行一系列的图像增强操作，包括颜色变换、平移缩放旋转和水平翻转。然后，它将增强后的图像和掩码添加到批次中。最后，它将批次中的图像和掩码转换为<code>np.float32</code>数据类型，并对图像和掩码进行归一化处理，将像素值缩放到[0, 1]的范围内。最后，使用<code>yield</code>语句将批次返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_generator</span>():</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(ids_train_split), batch_size):</span><br><span class="line">            x_batch = []</span><br><span class="line">            y_batch = []</span><br><span class="line">            end = <span class="built_in">min</span>(start + batch_size, <span class="built_in">len</span>(ids_train_split))</span><br><span class="line">            ids_train_batch = ids_train_split[start:end]</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ids_train_batch.values:</span><br><span class="line">                img = cv2.imread(<span class="string">&#x27;input/train/&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>)) <span class="comment">#读图片</span></span><br><span class="line">                img = cv2.resize(img, (input_size, input_size))</span><br><span class="line">                mask = cv2.imread(<span class="string">&#x27;input/train_masks/&#123;&#125;_mask.png&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>), cv2.IMREAD_GRAYSCALE) <span class="comment">#读掩码</span></span><br><span class="line">                mask = cv2.resize(mask, (input_size, input_size))</span><br><span class="line">                img = randomHueSaturationValue(img, <span class="comment">#HSV颜色空间变化与加强</span></span><br><span class="line">                                               hue_shift_limit=(-<span class="number">50</span>, <span class="number">50</span>),</span><br><span class="line">                                               sat_shift_limit=(-<span class="number">5</span>, <span class="number">5</span>),</span><br><span class="line">                                               val_shift_limit=(-<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">                img, mask = randomShiftScaleRotate(img, mask,<span class="comment"># 随机图像缩放平移旋转</span></span><br><span class="line">                                                   shift_limit=(-<span class="number">0.0625</span>, <span class="number">0.0625</span>),</span><br><span class="line">                                                   scale_limit=(-<span class="number">0.1</span>, <span class="number">0.1</span>),</span><br><span class="line">                                                   rotate_limit=(-<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">                img, mask = randomHorizontalFlip(img, mask) <span class="comment">#随机翻转</span></span><br><span class="line">                mask = np.expand_dims(mask, axis=<span class="number">2</span>)</span><br><span class="line">                x_batch.append(img)</span><br><span class="line">                y_batch.append(mask)</span><br><span class="line">            x_batch = np.array(x_batch, np.float32) / <span class="number">255</span></span><br><span class="line">            y_batch = np.array(y_batch, np.float32) / <span class="number">255</span></span><br><span class="line">            <span class="keyword">yield</span> x_batch, y_batch</span><br></pre></td></tr></table></figure>

<h4 id="e-训练"><a href="#e-训练" class="headerlink" title="e.训练"></a>e.训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.fit_generator(generator=train_generator(),</span><br><span class="line">                    steps_per_epoch=np.ceil(<span class="built_in">float</span>(<span class="built_in">len</span>(ids_train_split)) / <span class="built_in">float</span>(batch_size)),</span><br><span class="line">                    epochs=epochs,</span><br><span class="line">                    verbose=<span class="number">2</span>,</span><br><span class="line">                    callbacks=callbacks,</span><br><span class="line">                    validation_data=valid_generator(),</span><br><span class="line">                    validation_steps=np.ceil(<span class="built_in">float</span>(<span class="built_in">len</span>(ids_valid_split)) / <span class="built_in">float</span>(batch_size)))</span><br></pre></td></tr></table></figure>

<p>将这些作为参数传入训练器中，开始训练函数，训练出的模型保存在models文件及中。</p>
<h4 id="f-预测"><a href="#f-预测" class="headerlink" title="f. 预测"></a>f. 预测</h4><p>之后加载训练好的模型权重，把经过规范化的图片传入模型后获得预测结果，再调整预测的掩码，作用于原本的图片最终得到删除背景的照相馆汽车图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(filepath=<span class="string">&#x27;weights/best_weights.hdf5&#x27;</span>)  <span class="comment"># 加载预训练模型权重</span></span><br><span class="line"><span class="keyword">for</span> start <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(ids_test), batch_size)):</span><br><span class="line">    x_batch = []  <span class="comment"># 初始化空列表，用于存储批量输入图像</span></span><br><span class="line">    end = <span class="built_in">min</span>(start + batch_size, <span class="built_in">len</span>(ids_test))  <span class="comment"># 确定当前批次的结束索引</span></span><br><span class="line">    ids_test_batch = ids_test[start:end]  <span class="comment"># 获取测试ID的批次</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ids_test_batch.values:</span><br><span class="line">        img = cv2.imread(<span class="string">&#x27;input/test/&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>))  <span class="comment"># 读取输入的测试图像</span></span><br><span class="line">        img = cv2.resize(img, (input_size, input_size))  <span class="comment"># 调整图像大小以匹配模型的输入尺寸</span></span><br><span class="line">        x_batch.append(img)  <span class="comment"># 将预处理后的图像添加到批次列表中</span></span><br><span class="line">    x_batch = np.array(x_batch, np.float32) / <span class="number">255</span>  <span class="comment"># 将图像批次规范化</span></span><br><span class="line">    preds = model.predict_on_batch(x_batch)  <span class="comment"># 对图像批次进行预测</span></span><br><span class="line">    preds = np.squeeze(preds, axis=<span class="number">3</span>)  <span class="comment"># 从预测结果中移除冗余的维度</span></span><br><span class="line">    <span class="keyword">for</span> i, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">        prob = cv2.resize(pred, (orig_width, orig_height))  <span class="comment"># 调整预测掩膜的大小以匹配原始图像尺寸</span></span><br><span class="line">        mask = prob &gt; threshold  <span class="comment"># 基于概率阈值创建二值掩膜</span></span><br><span class="line">        img = cv2.imread(<span class="string">&#x27;input/test/&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(ids_test[start + i]))  <span class="comment"># 读取原始的测试图像</span></span><br><span class="line">        img = cv2.resize(img, (orig_width, orig_height))  <span class="comment"># 调整原始图像的大小以匹配掩膜大小</span></span><br><span class="line">        img[~mask] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  <span class="comment"># 将掩膜应用到原始图像上</span></span><br><span class="line">        cv2.imwrite(<span class="string">&#x27;output/&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(ids_test[start + i]), img)  <span class="comment"># 将掩膜图像保存到输出文件夹中</span></span><br><span class="line">        rle = run_length_encode(mask)  <span class="comment"># 将掩膜编码为Run-Length格式</span></span><br><span class="line">        rles.append(rle)  <span class="comment"># 将编码后的掩膜添加到RLE列表中</span></span><br></pre></td></tr></table></figure>

<h3 id="四、训练过程"><a href="#四、训练过程" class="headerlink" title="四、训练过程"></a>四、训练过程</h3><p>​	直接在本地运行代码，没有使用kaggle的服务器，所以条件限制就只训练了五轮。加载模型之后，开始每一轮的训练，由于kaggle的训练集过于大，所以每一轮都要花费较多时间和资源。</p>
<p><img src= "/gina511.github.io/img/loading.gif" data-src="/gina511.github.io/images/post-image/kaggle-masking-report/image-20240516200309746.png" alt="image-20240516200309746"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/5</span><br><span class="line">255/255 - 2536s - loss: 0.1780 - dice_coeff: 0.8944 - val_loss: 1.7487 - val_dice_coeff: 0.0485 - lr: 1.0000e-04 - 2536s/epoch - 10s/step</span><br><span class="line">Epoch 2/5</span><br><span class="line">255/255 - 2478s - loss: 0.0763 - dice_coeff: 0.9548 - val_loss: 0.0859 - val_dice_coeff: 0.9512 - lr: 1.0000e-04 - 2478s/epoch - 10s/step</span><br><span class="line">Epoch 3/5</span><br><span class="line">255/255 - 2485s - loss: 0.0517 - dice_coeff: 0.9707 - val_loss: 0.0404 - val_dice_coeff: 0.9770 - lr: 1.0000e-04 - 2485s/epoch - 10s/step</span><br><span class="line">Epoch 4/5</span><br><span class="line">255/255 - 2497s - loss: 0.0409 - dice_coeff: 0.9778 - val_loss: 0.0815 - val_dice_coeff: 0.9556 - lr: 1.0000e-04 - 2497s/epoch - 10s/step</span><br><span class="line">Epoch 5/5</span><br><span class="line">255/255 - 2479s - loss: 0.0353 - dice_coeff: 0.9813 - val_loss: 0.0298 - val_dice_coeff: 0.9844 - lr: 1.0000e-04 - 2479s/epoch - 10s/step</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>训练过程输出损失值变化和预测准确度</p>
<img src= "/gina511.github.io/img/loading.gif" data-src="/images/post-image/kaggle-masking-report/image-20240525134433622.png" alt="image-20240525134433622" style="zoom:67%;" />

<p>可以看到随着训练伦次的不断增多，训练集合和验证集的损失值都在降低，而由于训练一轮之后再根据验证集损失值调整参数，所以验证集初始化的损失值会比训练集低很多。</p>
<img src= "/gina511.github.io/img/loading.gif" data-src="/images/post-image/kaggle-masking-report/image-20240525134453902.png" alt="image-20240525134453902" style="zoom:67%;" />

<p>而随着训练伦次不断增多，模型预测的准确度也在上升，DICE coefficient是模型准确度的含义，越接近1说明预测的准确度也越高。也可以看到训练前期模型快速收敛，后期模型参数变化，损失值下降梯度都有减缓。</p>
<h3 id="五、输出图片"><a href="#五、输出图片" class="headerlink" title="五、输出图片"></a>五、输出图片</h3><p>原本kaggle给的预测图片很多，而课程实验中老师做了删减，只保留了40张需要删除背景的图片，于是在上述过程中，真正模型预测的时候代码运行很快。</p>
<p>并且kaggle提交的是掩码的csv文件，csv文件中是掩码经过浮点编码之后保存的数值，不具有可读性，虽然最终的评价指标还是参考损失值，准确度等等，但是为了让算法实现可用性，就修改了代码，将生成的csv中的模型预测掩码作用于原本的输入图像中。</p>
<img src= "/gina511.github.io/img/loading.gif" data-src="/images/post-image/kaggle-masking-report/image-20240525135327471.png" alt="image-20240525135327471" style="zoom:67%;" />

<p>原始图像</p>
<img src= "/gina511.github.io/img/loading.gif" data-src="/images/post-image/kaggle-masking-report/image-20240525135350825.png" alt="image-20240525135350825" style="zoom:67%;" />

<p>掩码遮蔽图像</p>
<p>可以看到仅仅训练5轮的Unet模型也能很好的将汽车图片中的照相馆背景删除。Unet模型对于背景和目标物的特征提取能力真的很强。</p>
<h3 id="六、实验收获"><a href="#六、实验收获" class="headerlink" title="六、实验收获"></a>六、实验收获</h3><p>之前的讨论课文献阅读中经常提到Unet这个网络结构，但是一直对这个的了解并不深入，完成kaggle的实验之后对Unet的原理和实现的过程了解更加深入了，并且对于keras这个深度学习框架的使用了解也更加深入。并且上课老师也一直提到图像增强的方式，完成kaggle实验的时候参考了很多网上的代码，发现了很多图像增强的方法，包括平移，旋转，缩放，翻转以及它们的具体实现，写代码的能力也up了。</p>
</div>
        
  </div>

  <div class="share-reward">
    <div class="share">
        
<div class="social-share" data-sites="facebook,twitter,wechat,weibo,qq"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script>


      </div>
        <div class="reward">
          
        </div>
    </div>
    
    <div class="post_tags">
      
        <i class="fas fa-tag"></i> <a href="/gina511.github.io/tags/UNET/" class="tag">UNET</a>
      
        <i class="fas fa-tag"></i> <a href="/gina511.github.io/tags/KAGGLE/" class="tag">KAGGLE</a>
      
        <i class="fas fa-tag"></i> <a href="/gina511.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" class="tag">图像分割</a>
      
        <i class="fas fa-tag"></i> <a href="/gina511.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" class="tag">计算机视觉</a>
      
    </div>
    <div class="post-nav">
      
        <div class="post-nav-prev post-nav-item">
            <a href="/gina511.github.io/2024/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%86%B3%E7%AD%96%E6%A0%91/" >基于ID3算法和后剪枝的决策树实验<i class="fa fa-chevron-left"></i></a>
        </div>
      
      
        <div class="post-nav-next post-nav-item">
            <a href="/gina511.github.io/2024/04/12/gpt%E8%B0%83%E7%94%A8/" >NLP接口调用记录<i class="fa fa-chevron-right"></i></a>
        </div>
      
    </div>
      



  <div id="valine"></div>
  <script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script>
  <script>
        new Valine({
          el: '#valine',
          appId: "",
          appKey: "",
          avatar: "mm",
          lang: "",
          meta: 'nick,mail,link'.split(','),
          requiredFields: 'nick,mail'.split(','),
          placeholder: "评论记得带上邮箱,你的留言我会马上收到邮箱提醒哒",
          pageSize:'10',
          recordIP: 'false',
          serverURLs: "",
          emojiCDN: "",
          enableQQ: "true",
        });
  </script>
  

</article>

    
<a id="gotop" href="javascript:" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    






    
<div id="bottom-outer">
    <div id="bottom-inner">
        © 2020 <i class="fa fa-heart" id="heart"></i> Gina 
        <br>
        Powered by 
        <a target="_blank" rel="noopener" href="http://hexo.io">hexo</a> | Theme is <a target="_blank" rel="noopener" href="https://github.com/a2396837/hexo-theme-blank/">blank</a>
        
          <div class="icp-info">
            
          <a href="" target="_blank"> </a>
        </div>
        
    </div>  
  </div>
  
<script src="https://cdn.jsdelivr.net/npm/layui-src@2.5.5/dist/layui.min.js"></script>



  
    <script src="/gina511.github.io/js/script.js"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script>
  

 



  <script>
    window.lazyLoadOptions = {
      elements_selector: 'img',
      threshold: 0
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script>   
  


  <script>
    var images = $('img').not('.nav-logo img').not('.card img').not($('a>img')).not('.reward-content img')
    images.each(function (i, o) {
      var lazyloadSrc = $(o).attr('data-src') ? $(o).attr('data-src') : $(o).attr('src')
      $(o).wrap(`<a href="${lazyloadSrc}" data-fancybox="group" data-caption="${$(o).attr('alt')}" class="fancybox"></a>`)
    })
  </script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script>
  <script>
        $().fancybox({
      selector: '[data-fancybox]',
      loop: true,
      transitionEffect: 'slide',
      protect: true,
      buttons: ['slideShow', 'fullScreen', 'thumbs', 'close']
    })
  </script>   
  






  <script>
    jQuery(document).ready(function () {
      var QRBox = $('#QRBox');
      var MainBox = $('#MainBox');
      var AliPayQR = '';
      var WeChanQR = '';

      function showQR(QR) {
        if (QR) {
          MainBox.css('background-image', 'url(' + QR + ')');
        }
        $('#donateBox').addClass('blur');
        QRBox.fadeIn(300, function (argument) {
          MainBox.addClass('showQR');
        });
      }

      $('#donateBox>li').click(function (event) {
        var thisID = $(this).attr('id');
        if (thisID === 'AliPay') {
          showQR(AliPayQR);
        } else if (thisID === 'WeChat') {
          showQR(WeChanQR);
        }
      });

      MainBox.click(function (event) {
        MainBox.removeClass('showQR').addClass('hideQR');
        setTimeout(function (a) {
          QRBox.fadeOut(300, function (argument) {
            MainBox.removeClass('hideQR');
          });
          $('#DonateText,#donateBox,#github').removeClass('blur');
        }, 600);

      });
    });
  </script>
  




  <script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script>
  




  
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js"></script>
<script>
!function (e, t, a) {
  var initCopyCode = function(){
    var copyHtml = '';
    copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
    copyHtml += '  <i class="fa fa-clipboard"></i><span>复制</span>';
    copyHtml += '</button>';
    $(".highlight .code pre").before(copyHtml);
    new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
  }
  initCopyCode();
}(window, document);
</script>  
  

<script>
  var btntop = $('#gotop');
  btntop.on('click', function (e) {
    e.preventDefault();
    $('html, body').animate({ scrollTop: 0 }, '300');
  });

  var $table = $('.content table').not($('figure.highlight > table'))
$table.each(function () {
  $(this).wrap('<div class="table-wrap"></div>')
})
</script>



</html>